# -*- coding: utf-8 -*-
"""pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n4ptowcB9hLE13krJcPOPfQL_qz-LqRk

https://www.kaggle.com/datasets/purusinghvi/email-spam-classification-dataset?resource=download

Nama              : Geraldy Yobel Primandaru

Username dicoding :geraldyyobel

<style>
table, tr, td {
  border: 1px solid black;
  border-radius: 10px;
}

</style>
<table>
  <tr>
    <th></th>
    <th>Deskripsi</th>
  </tr>
  <tr>
    <td>Dataset</td>
    <td>https://www.kaggle.com/datasets/purusinghvi/email-spam-classification-dataset?resource=download</td>
  </tr>
  <tr>
    <td>Masalah</td>
    <td>masalah utama adalah memastikan bahwa model klasifikasi email dapat membedakan dengan tepat antara email yang merupakan spam dan yang bukan spam.</td>
  </tr>
  <tr>
    <td>Solusi Machine Learning</td>
    <td>membangun sebuah machine learning pipeline menggunakan TensorFlow Extended (TFX) untuk melakukan klasifikasi email spam berdasarkan dataset yang diberikan</td>
  </tr>
  <tr>
    <td>Metode Pengolahan</td>
    <td>metode normalisasi menjadi huruf kecil menggunakan fungsi tf.strings.lower() dari TensorFlow. Ini dilakukan untuk memastikan konsistensi dalam representasi teks, sehingga kata-kata yang sama dengan huruf besar atau huruf kecil dapat dianggap setara. kemudian Tanda baca dihilangkan dari teks email menggunakan pendekatan sederhana dengan tidak menyertakan tanda baca saat normalisasi.</td>
  </tr>
  <tr>
    <td>Arsitektur Model</td>
    <td>1. Normalisasi Teks, 2. Layer Embeding, 3. Layer Conv1D, 4. Dense Layer, 5. Output Layer</td>
  </tr>
  <tr>
    <td>Metrik Evaluasi</td>
    <td>1. binary accuracy dan 2. validation biner accuracy</td>
  </tr>
  <tr>
    <td>Performa Model</td>
    <td>Model biner accuracy sebesar 95.03% pada data pelatihan. Ini menunjukkan bahwa model memiliki kinerja yang sangat baik dalam membedakan antara email spam dan non-spam pada data yang digunakan untuk melatihnya. kemudian Performa pada Data Validasi: Model mencapai akurasi biner validasi sebesar 92.08% pada data validasi. Ini menunjukkan bahwa model juga memiliki kinerja yang baik./td>
  </tr>
</table>

Machine Learning Pipeline
"""

# !pip install -U tfx
# # !pip install jupyter scikit-learn tensorflow tfx
# !pip install jupyter scikit-learn tensorflow tfx==1.11.0 flask joblib

!mkdir data

#import library
import tensorflow as tf
from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator, Transform, Trainer, Tuner
from tfx.proto import example_gen_pb2
from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext
import os
import pandas as pd
import tensorflow_transform as tft
from tensorflow.keras import layers
import tensorflow_hub as hub
from tfx.components.trainer.fn_args_utils import FnArgs

"""## Set Variable"""

PIPELINE_NAME = "sarcasm-pipeline"
SCHEMA_PIPELINE_NAME = "sarcasm-tfdv-schema"

#Directory untuk menyimpan artifact yang akan dihasilkan
PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)

# Path to a SQLite DB file to use as an MLMD storage.
METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')

# Output directory where created models from the pipeline will be exported.
SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)

# from absl import logging
# logging.set_verbosity(logging.INFO)

# data_path = pd.read_csv("/content/data.csv")
data_path = pd.read_csv("combined_data.csv")
dataa = 'data'
import csv
import sys

# Set batas ukuran bidang yang lebih besar
csv.field_size_limit(sys.maxsize)

# Lakukan parsing CSV seperti biasa
with open('/content/combined_data.csv', 'r') as file:
    reader = csv.reader(file)
    for row in reader:
        # Lakukan sesuatu dengan setiap baris
        pass

# data_path = data_path.rename(columns={'is_sarcastic':'is_sarcastic'})
data_path = data_path.rename(columns={'label':'label'})
# data_path.to_csv(os.path.join(dataa, "data.csv"), index = False)
data_path.to_csv(os.path.join(dataa, "combined_data.csv"), index = False)

DATA_ROOT = "data"

data_path

DATA_ROOT

interactive_context = InteractiveContext(pipeline_root=PIPELINE_ROOT)

"""## Data Ingestion"""

output = example_gen_pb2.Output(
    split_config = example_gen_pb2.SplitConfig(splits=[
        example_gen_pb2.SplitConfig.Split(name="train", hash_buckets=8),
        example_gen_pb2.SplitConfig.Split(name="eval", hash_buckets=2)
    ])
)
example_gen = CsvExampleGen(input_base=DATA_ROOT, output_config=output)

interactive_context.run(example_gen)

"""## Data Validation"""

statistics_gen = StatisticsGen(
    examples=example_gen.outputs["examples"]
)


interactive_context.run(statistics_gen)

interactive_context.show(statistics_gen.outputs["statistics"])

schema_gen = SchemaGen(statistics=statistics_gen.outputs["statistics"]
)


interactive_context.run(schema_gen)

interactive_context.show(schema_gen.outputs["schema"])

example_validator = ExampleValidator(
    statistics=statistics_gen.outputs['statistics'],
    schema=schema_gen.outputs['schema']
)

interactive_context.run(example_validator)

interactive_context.show(example_validator.outputs['anomalies'])

"""## Data Preprocessing"""

TRANSFORM_MODULE_FILE = "sarcasm_transform.py"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {TRANSFORM_MODULE_FILE}
# 
# import tensorflow as tf
# 
# 
# #LABEL_KEY = "is_sarcastic"
# #FEATURE_KEY = "headline"
# LABEL_KEY = "label"
# FEATURE_KEY = "text"
# 
# def transformed_name(key):
#     """Renaming transformed features"""
#     return key + "_xf"
# 
# def preprocessing_fn(inputs):
#     """
#     Preprocess input features into transformed features
# 
#     Args:
#         inputs: map from feature keys to raw features.
# 
#     Return:
#         outputs: map from feature keys to transformed features.
#     """
# 
#     outputs = {}
# 
#     outputs[transformed_name(FEATURE_KEY)] = tf.strings.lower(inputs[FEATURE_KEY])
# 
#     outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)
# 
#     return outputs

transform  = Transform(
    examples=example_gen.outputs['examples'],
    schema= schema_gen.outputs['schema'],
    module_file=os.path.abspath(TRANSFORM_MODULE_FILE)
)

interactive_context.run(transform)

"""# Latihan Pembuatan Komponen Pengembangan dan Validasi Model dalam Machine Learning Pipeline

## Membuat Tahapan Pengembangan Model

### Training function
"""

TRAINER_MODULE_FILE = "sarcasm_trainer.py"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile {TRAINER_MODULE_FILE}
# import tensorflow as tf
# import tensorflow_transform as tft
# from tensorflow.keras import layers
# import os
# import tensorflow_hub as hub
# from tfx.components.trainer.fn_args_utils import FnArgs
# 
# #LABEL_KEY = "is_sarcastic"
# #FEATURE_KEY = "headline"
# LABEL_KEY = "label"
# FEATURE_KEY = "text"
# 
# def transformed_name(key):
#     """Renaming transformed features"""
#     return key + "_xf"
# 
# def gzip_reader_fn(filenames):
#     """Loads compressed data"""
#     return tf.data.TFRecordDataset(filenames, compression_type='GZIP')
# 
# 
# def input_fn(file_pattern,
#             tf_transform_output,
#             num_epochs,
#             batch_size=64)->tf.data.Dataset:
#     """Get post_tranform feature & create batches of data"""
# 
#     # Get post_transform feature spec
#     transform_feature_spec = (
#         tf_transform_output.transformed_feature_spec().copy())
# 
#     # create batches of data
#     dataset = tf.data.experimental.make_batched_features_dataset(
#         file_pattern=file_pattern,
#         batch_size=batch_size,
#         features=transform_feature_spec,
#         reader=gzip_reader_fn,
#         num_epochs=num_epochs,
#         label_key = transformed_name(LABEL_KEY))
#     return dataset
# 
# # os.environ['TFHUB_CACHE_DIR'] = '/hub_chace'
# # embed = hub.KerasLayer("https://tfhub.dev/google/universal-sentence-encoder/4")
# 
# # Vocabulary size and number of words in a sequence.
# VOCAB_SIZE = 10000
# SEQUENCE_LENGTH = 100
# 
# vectorize_layer = layers.TextVectorization(
#     standardize="lower_and_strip_punctuation",
#     max_tokens=VOCAB_SIZE,
#     output_mode='int',
#     output_sequence_length=SEQUENCE_LENGTH)
# 
# 
# embedding_dim=16
# def model_builder():
#     """Build machine learning model"""
#     inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)
#     reshaped_narrative = tf.reshape(inputs, [-1])
#     x = vectorize_layer(reshaped_narrative)
#     x = layers.Embedding(VOCAB_SIZE, embedding_dim, name="embedding")(x)
#     x = layers.GlobalAveragePooling1D()(x)
#     x = layers.Dense(64, activation='relu')(x)
#     x = layers.Dense(32, activation="relu")(x)
#     outputs = layers.Dense(1, activation='sigmoid')(x)
# 
# 
#     model = tf.keras.Model(inputs=inputs, outputs = outputs)
# 
#     model.compile(
#         loss = 'binary_crossentropy',
#         optimizer=tf.keras.optimizers.Adam(0.01),
#         metrics=[tf.keras.metrics.BinaryAccuracy()]
# 
#     )
# 
#     # print(model)
#     model.summary()
#     return model
# 
# 
# def _get_serve_tf_examples_fn(model, tf_transform_output):
# 
#     model.tft_layer = tf_transform_output.transform_features_layer()
# 
#     @tf.function
#     def serve_tf_examples_fn(serialized_tf_examples):
# 
#         feature_spec = tf_transform_output.raw_feature_spec()
# 
#         feature_spec.pop(LABEL_KEY)
# 
#         parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)
# 
#         transformed_features = model.tft_layer(parsed_features)
# 
#         # get predictions using the transformed features
#         return model(transformed_features)
# 
#     return serve_tf_examples_fn
# 
# def run_fn(fn_args: FnArgs) -> None:
# 
#     log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')
# 
#     tensorboard_callback = tf.keras.callbacks.TensorBoard(
#         log_dir = log_dir, update_freq='batch'
#     )
# 
#     es = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', mode='max', verbose=1, patience=10)
#     mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True)
# 
# 
#     # Load the transform output
#     tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)
# 
#     # Create batches of data
#     train_set = input_fn(fn_args.train_files, tf_transform_output, 10)
#     val_set = input_fn(fn_args.eval_files, tf_transform_output, 10)
#     vectorize_layer.adapt(
#         [j[0].numpy()[0] for j in [
#             i[0][transformed_name(FEATURE_KEY)]
#                 for i in list(train_set)]])
# 
#     # Build the model
#     model = model_builder()
# 
# 
#     # Train the model
#     model.fit(x = train_set,
#             validation_data = val_set,
#             callbacks = [tensorboard_callback, es, mc],
#             steps_per_epoch = 1000,
#             validation_steps= 1000,
#             epochs=10)
#     signatures = {
#         'serving_default':
#         _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(
#                                     tf.TensorSpec(
#                                     shape=[None],
#                                     dtype=tf.string,
#                                     name='examples'))
#     }
#     model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)

"""### Component Trainer"""

from tfx.proto import trainer_pb2

trainer  = Trainer(
    module_file=os.path.abspath(TRAINER_MODULE_FILE),
    examples = transform.outputs['transformed_examples'],
    transform_graph=transform.outputs['transform_graph'],
    schema=schema_gen.outputs['schema'],
    train_args=trainer_pb2.TrainArgs(splits=['train']),
    eval_args=trainer_pb2.EvalArgs(splits=['eval'])
)
interactive_context.run(trainer)

"""## Membuat Tahapan Analisis dan Validasi Model

### Component Resolver
"""

from tfx.dsl.components.common.resolver import Resolver
from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy
from tfx.types import Channel
from tfx.types.standard_artifacts import Model, ModelBlessing

model_resolver = Resolver(
    strategy_class= LatestBlessedModelStrategy,
    model = Channel(type=Model),
    model_blessing = Channel(type=ModelBlessing)
).with_id('Latest_blessed_model_resolver')

interactive_context.run(model_resolver)

"""### Component Evaluator"""

import tensorflow_model_analysis as tfma

eval_config = tfma.EvalConfig(
    # model_specs=[tfma.ModelSpec(label_key='is_sarcastic')],
    model_specs=[tfma.ModelSpec(label_key='label')],
    slicing_specs=[tfma.SlicingSpec()],
    metrics_specs=[
        tfma.MetricsSpec(metrics=[

            tfma.MetricConfig(class_name='ExampleCount'),
            tfma.MetricConfig(class_name='AUC'),
            tfma.MetricConfig(class_name='FalsePositives'),
            tfma.MetricConfig(class_name='TruePositives'),
            tfma.MetricConfig(class_name='FalseNegatives'),
            tfma.MetricConfig(class_name='TrueNegatives'),
            tfma.MetricConfig(class_name='BinaryAccuracy',
                threshold=tfma.MetricThreshold(
                    value_threshold=tfma.GenericValueThreshold(
                        lower_bound={'value':0.5}),
                    change_threshold=tfma.GenericChangeThreshold(
                        direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                        absolute={'value':0.0001})
                    )
            )
        ])
    ]

)

from tfx.components import Evaluator
evaluator = Evaluator(
    examples=example_gen.outputs['examples'],
    model=trainer.outputs['model'],
    baseline_model=model_resolver.outputs['model'],
    eval_config=eval_config)

interactive_context.run(evaluator)

"""### Result: interactive visualization"""

# Visualize the evaluation results
eval_result = evaluator.outputs['evaluation'].get()[0].uri
tfma_result = tfma.load_eval_result(eval_result)
tfma.view.render_slicing_metrics(tfma_result)
tfma.addons.fairness.view.widget_view.render_fairness_indicator(
    tfma_result
)

"""# Latihan Model Deployment

## Component Pusher
"""

from tfx.components import Pusher
from tfx.proto import pusher_pb2

pusher = Pusher(
model=trainer.outputs['model'],
model_blessing=evaluator.outputs['blessing'],
push_destination=pusher_pb2.PushDestination(
    filesystem=pusher_pb2.PushDestination.Filesystem(
        base_directory='serving_model_dir/sarchasm-detection-model'))

)

interactive_context.run(pusher)

# !nohup tensorflow_model_server --port=1007 --model_name=sarchasm-detection-model --model_base_path=/path/to/your/model > server.log 2>&1 &
!nohup tensorflow_model_server --port=4040 --model_name=sarchasm-detection-model --model_base_path=/path/to/your/model > server.log 2>&1 &

# get_ipython().system_raw('./ngrok http 1007 &')
get_ipython().system_raw('./ngrok http 4040 &')

!curl -s http://localhost:4040/api/tunnels | python3 -c \
    "import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])"

! pip install pyngrok

! ngrok authtoken 2fRtsqKKg97OYHEgXIiUCdRkbCR_2f7XjWBpNyMXXm9rKFqeousr_2fRtsuHjaywC5zuliUJqL6UqjBe

!ngrok config add-authtoken 2fRtsqKKg97OYHEgXIiUCdRkbCR_2f7XjWBpNyMXXm9rKFqeo

!authtoken: 2fRtsqKKg97OYHEgXIiUCdRkbCR_2f7XjWBpNyMXXm9rKFqeo

!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip ngrok-stable-linux-amd64.zip

get_ipython().system_raw('tensorboard --logdir /content/trainingdata/objectdetection/ckpt_output/trainingImatges/ --host 0.0.0.0 --port 6006 &')

get_ipython().system_raw('./ngrok http 6006 &')

!curl -s http://localhost:4040/api/tunnels | python3 -c \
 "import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])"

from pyngrok import ngrok

# Start the HTTP tunnel using ngrok
http_tunnel = ngrok.connect(addr="4040", proto="http")

# Print the public URL of the tunnel
print("Public URL:", http_tunnel.public_url)

# import requests
# from pprint import PrettyPrinter

# pp = PrettyPrinter()

# try:
#     response = requests.get("https://428f-34-29-135-154.ngrok-free.app")
#     response.raise_for_status()  # Raise an exception for 4xx and 5xx status codes
#     if response.text:  # Check if the response body is not empty
#         pp.pprint(response.json())
#     else:
#         print("Response body is empty.")
# except requests.exceptions.RequestException as e:
#     print("Error:", e)